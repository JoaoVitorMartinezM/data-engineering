# Delta Lake Time Travel - Guia de Uso

## ğŸ¯ O que foi implementado

ConfiguraÃ§Ã£o completa do **Delta Lake 3.2.0** com **Apache Spark 3.5.7** para suportar **Time Travel** em um ambiente Docker com MinIO como storage S3.

## ğŸ”§ Componentes

1. **Imagem Docker customizada** (`spark-delta:3.5.7`):
   - Spark 3.5.7 com Python 3
   - Delta Lake 3.2.0 (JARs prÃ©-instalados)
   - Hadoop AWS 3.3.4
   - AWS SDK Bundle 1.12.262

2. **Script de Time Travel** (`delta_timetravel.py`):
   - Escrita inicial de dados (versÃ£o 0)
   - AtualizaÃ§Ã£o de registros (versÃ£o 1)
   - Leitura da versÃ£o atual
   - **Time Travel** para versÃµes anteriores
   - VisualizaÃ§Ã£o do histÃ³rico de versÃµes

## ğŸš€ Como executar

### 1. Certifique-se de que os containers estÃ£o rodando

```powershell
docker-compose up -d spark-master spark-worker minio
```

### 2. Execute o script dentro do container Spark

```powershell
docker exec spark_master python3 /opt/spark/work-dir/delta_timetravel.py
```

## ğŸ“Š O que o script faz

### VersÃ£o 0 (Dados originais)
```
+-------+---+
|   name| id|
+-------+---+
|  Alice|  1|
|    Bob|  2|
|Charlie|  3|
+-------+---+
```

### VersÃ£o 1 (ApÃ³s update)
```
+-------+---+
|   name| id|
+-------+---+
|  Alice|  1|
| Robert|100|  â† Bob foi atualizado!
|Charlie|  3|
+-------+---+
```

### Time Travel - Lendo VersÃ£o 0
```python
# Mesmo apÃ³s o update, vocÃª pode ler a versÃ£o antiga
df_old = spark.read.format("delta").option("versionAsOf", 0).load(DELTA_PATH)
# Retorna os dados originais com "Bob" em vez de "Robert"
```

### HistÃ³rico de versÃµes
```
+-------+-------------------+---------+
|version|timestamp          |operation|
+-------+-------------------+---------+
|1      |2025-11-21 14:03:18|UPDATE   |
|0      |2025-11-21 14:03:08|WRITE    |
+-------+-------------------+---------+
```

## ğŸ” Funcionalidades do Delta Lake

### Time Travel por VersÃ£o
```python
# Ler uma versÃ£o especÃ­fica
df = spark.read.format("delta").option("versionAsOf", 0).load(path)
```

### Time Travel por Timestamp
```python
# Ler dados de um momento especÃ­fico
df = spark.read.format("delta")\
    .option("timestampAsOf", "2025-11-21 14:00:00")\
    .load(path)
```

### Consultar HistÃ³rico
```python
from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, path)
deltaTable.history().show()
```

### OperaÃ§Ãµes ACID
```python
# Update
deltaTable.update(
    condition="name = 'Bob'",
    set={"name": "'Robert'", "id": "100"}
)

# Delete
deltaTable.delete("id > 100")

# Merge (Upsert)
deltaTable.merge(
    source_df,
    "target.id = source.id"
).whenMatchedUpdate(set={"name": "source.name"})\
 .whenNotMatchedInsert(values={"id": "source.id", "name": "source.name"})\
 .execute()
```

## ğŸ—‚ï¸ Estrutura dos Arquivos

```
intro-de/
â”œâ”€â”€ delta_timetravel.py         # Script principal de Time Travel
â”œâ”€â”€ delta_timetravel_simple.py  # VersÃ£o simplificada do script
â”œâ”€â”€ Dockerfile.spark-delta      # Dockerfile customizado com Delta Lake
â”œâ”€â”€ docker-compose.yaml         # ConfiguraÃ§Ã£o dos containers
â””â”€â”€ datalake/
    â””â”€â”€ spark-timetravel/
        â””â”€â”€ test/                # Pasta onde os dados Delta sÃ£o salvos
            â”œâ”€â”€ _delta_log/      # Transaction log do Delta Lake
            â””â”€â”€ *.parquet        # Arquivos de dados
```

## ğŸ› ï¸ Reconstruir a imagem (se necessÃ¡rio)

Se vocÃª precisar modificar o Dockerfile:

```powershell
docker build -t spark-delta:3.5.7 -f Dockerfile.spark-delta .
docker-compose restart spark-master spark-worker
```

## ğŸ”— Compatibilidade das VersÃµes

| Componente | VersÃ£o | Motivo |
|------------|--------|--------|
| Spark | 3.5.7 | VersÃ£o estÃ¡vel atual |
| Delta Lake | 3.2.0 | CompatÃ­vel com Spark 3.5.x |
| Scala | 2.12 | VersÃ£o padrÃ£o do Spark 3.5.7 |
| Hadoop AWS | 3.3.4 | CompatÃ­vel com Spark 3.5.x |
| AWS SDK | 1.12.262 | Recomendado para Hadoop 3.3.4 |

## ğŸ“ Acessar o MinIO

- Console: http://localhost:9001
- UsuÃ¡rio: `admin`
- Senha: `password`

VocÃª pode visualizar os arquivos Delta criados no bucket `spark-timetravel/test/`.

## âš ï¸ Notas Importantes

1. **Os JARs estÃ£o prÃ©-instalados**: NÃ£o Ã© necessÃ¡rio especificar `spark.jars.packages` no cÃ³digo
2. **Execute dentro do container**: O PySpark sÃ³ estÃ¡ disponÃ­vel dentro do container
3. **Dados persistentes**: Os dados ficam salvos em `./datalake/spark-timetravel/`
4. **MinIO deve estar rodando**: O script precisa do MinIO para funcionar como storage S3

## ğŸ“ Recursos Adicionais

- [DocumentaÃ§Ã£o Delta Lake](https://docs.delta.io/)
- [Delta Lake Time Travel](https://docs.delta.io/latest/delta-batch.html#-deltatimetravel)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
