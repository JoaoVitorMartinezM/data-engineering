# DAGs do Airflow para Pipeline ETL de Cintilografia

## üìã DAGs Criadas

### 1. **etl_simple_dag.py** - DAG Simples ‚≠ê (Recomendada para come√ßar)
- **ID**: `etl_cintilografia_simple`
- **Execu√ß√£o**: Manual (sem agendamento)
- **Tasks**: 3 tasks b√°sicas
- **Descri√ß√£o**: Pipeline ETL simples e direto

**Fluxo:**
```
Bronze Script ‚Üí Silver Script ‚Üí Data Check
```

### 2. **etl_cintilografia_dag.py** - DAG Intermedi√°ria 
- **ID**: `etl_cintilografia_pipeline`
- **Execu√ß√£o**: Di√°ria (@daily)
- **Tasks**: 5 tasks com verifica√ß√£o de qualidade
- **Descri√ß√£o**: Pipeline com verifica√ß√µes b√°sicas de qualidade

**Fluxo:**
```
Start ‚Üí Bronze ‚Üí Silver ‚Üí Quality Check ‚Üí End
```

### 3. **etl_cintilografia_advanced_dag.py** - DAG Avan√ßada üöÄ
- **ID**: `etl_cintilografia_pipeline_advanced`
- **Execu√ß√£o**: Di√°ria √†s 2h da manh√£ (0 2 * * *)
- **Tasks**: 10+ tasks com funcionalidades avan√ßadas
- **Descri√ß√£o**: Pipeline completo com backup, relat√≥rios e monitoramento

**Fluxo:**
```
Start ‚Üí Prerequisites ‚Üí Bronze ‚Üí Silver ‚Üí Backup ‚Üí Quality ‚Üí Report ‚Üí End
```

## üöÄ Como Executar

### 1. Verificar se as DAGs est√£o no Airflow

Acesse o Airflow em: **http://localhost:8082**
- Usu√°rio: `admin`
- Senha: `admin`

### 2. Copiar Scripts para o Spark

Certifique-se que os scripts est√£o no container:

```powershell
docker cp bronze_script.py spark_master:/opt/spark/work-dir/
docker cp silver_script.py spark_master:/opt/spark/work-dir/
```

### 3. Executar DAG Simples (Recomendado)

1. No Airflow Web UI, procure por: `etl_cintilografia_simple`
2. Ative a DAG clicando no toggle
3. Clique em "Trigger DAG" para executar manualmente

### 4. Monitorar Execu√ß√£o

- Clique na DAG para ver o gr√°fico de tasks
- Clique em cada task para ver logs
- Verde = Sucesso, Vermelho = Erro

## üîß Configura√ß√£o dos Scripts

### Antes de executar, certifique-se que:

1. **MinIO est√° rodando**: `docker-compose ps minio`
2. **Spark est√° rodando**: `docker-compose ps spark-master`
3. **Scripts est√£o no container**: 
   ```bash
   docker exec spark_master ls -la /opt/spark/work-dir/
   ```

## üìä O que cada Task faz

### Bronze Layer (`run_bronze_script`)
- Baixa dados do GitHub
- Limpa nomes de colunas
- Salva no formato Delta em `s3a://bronze/dados_cintilografia`

### Silver Layer (`run_silver_script`)
- L√™ dados do Bronze
- Aplica transforma√ß√µes e limpezas
- Salva dados processados em `s3a://silver/dados_cintilografia`

### Data Check (`check_data`)
- Verifica se os dados foram salvos corretamente
- Conta registros em cada layer
- Valida integridade dos dados

## ‚ö†Ô∏è Troubleshooting

### Problema: DAG n√£o aparece no Airflow
```bash
# Reiniciar o scheduler
docker-compose restart airflow-scheduler
```

### Problema: Task falha com "container not found"
```bash
# Verificar se containers est√£o rodando
docker-compose ps

# Reiniciar containers se necess√°rio
docker-compose restart spark-master
```

### Problema: Erro de conex√£o S3
```bash
# Verificar se MinIO est√° acess√≠vel
curl http://localhost:9000/minio/health/live
```

### Problema: Scripts n√£o encontrados
```bash
# Copiar scripts novamente
docker cp bronze_script.py spark_master:/opt/spark/work-dir/
docker cp silver_script.py spark_master:/opt/spark/work-dir/

# Verificar se foram copiados
docker exec spark_master ls -la /opt/spark/work-dir/
```

## üìà Exemplo de Execu√ß√£o Bem-Sucedida

```
‚úÖ run_bronze_script: Sucesso (2 min 30s)
   - 106 registros processados
   - Dados salvos em s3a://bronze/dados_cintilografia

‚úÖ run_silver_script: Sucesso (1 min 15s)
   - Transforma√ß√µes aplicadas
   - Dados salvos em s3a://silver/dados_cintilografia

‚úÖ check_data: Sucesso (30s)
   - Bronze Layer: 106 registros
   - Silver Layer: 106 registros
```

## üéØ Pr√≥ximos Passos

1. **Comece com a DAG simples** para testar
2. **Monitore os logs** para entender o comportamento
3. **Customize as DAGs** conforme suas necessidades
4. **Adicione notifica√ß√µes** por email/Slack
5. **Implemente testes de qualidade** mais avan√ßados

## üîó Links √öteis

- **Airflow UI**: http://localhost:8082
- **MinIO Console**: http://localhost:9001
- **Spark Master UI**: http://localhost:8080

## üìù Logs Importantes

Para debugar problemas, verifique os logs:
```bash
# Logs do Airflow
docker logs airflow_webserver
docker logs airflow_scheduler

# Logs do Spark
docker logs spark_master

# Logs das tasks (via Airflow UI)
```