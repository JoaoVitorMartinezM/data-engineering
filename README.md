# ğŸš€ Data Engineering Infrastructure - Guia Completo

## ğŸ“‹ VisÃ£o Geral

Este projeto implementa uma infraestrutura completa de Data Engineering usando Docker Compose, incluindo:

- **Apache Spark** com Delta Lake para processamento de dados
- **Apache Airflow** para orquestraÃ§Ã£o de pipelines
- **MinIO** como storage S3-compatible
- **Apache Kafka** para streaming de dados
- **OpenMetadata** para data governance
- **PostgreSQL** como backend do Airflow

## ğŸ—ï¸ Arquitetura da Infraestrutura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Apache Spark  â”‚    â”‚ Apache Airflow  â”‚    â”‚     MinIO       â”‚
â”‚   (3.5.7)       â”‚â—„â”€â”€â–ºâ”‚   (2.8.1)       â”‚â—„â”€â”€â–ºâ”‚   (S3 Storage)  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Master:8080   â”‚    â”‚ â€¢ UI:8082       â”‚    â”‚ â€¢ UI:9001       â”‚
â”‚ â€¢ Worker:8081   â”‚    â”‚ â€¢ API:8082      â”‚    â”‚ â€¢ API:9000      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Apache Kafka  â”‚    â”‚  OpenMetadata   â”‚    â”‚   PostgreSQL    â”‚
â”‚   (7.5.0)       â”‚    â”‚   (1.10.7)      â”‚    â”‚     (13)        â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Port:9093     â”‚    â”‚ â€¢ UI:8585       â”‚    â”‚ â€¢ Port:5432     â”‚
â”‚ â€¢ Zookeeper:2181â”‚    â”‚ â€¢ API:8586      â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ PrÃ©-requisitos

### Requisitos de Sistema
- **Docker**: VersÃ£o 20.10+
- **Docker Compose**: VersÃ£o 2.0+
- **RAM**: MÃ­nimo 8GB (Recomendado 16GB)
- **Disk Space**: MÃ­nimo 10GB livres
- **CPU**: MÃ­nimo 4 cores

### Verificar InstalaÃ§Ã£o
```powershell
# Verificar Docker
docker --version
docker-compose --version

# Verificar recursos disponÃ­veis
docker system info
```

## ğŸ“¦ PreparaÃ§Ã£o do Ambiente

### 1. Clonar ou Baixar o Projeto
```powershell
git clone <repository-url>
cd intro-de
```

### 2. Criar Estrutura de DiretÃ³rios
```powershell
# O docker-compose criarÃ¡ automaticamente, mas vocÃª pode criar manualmente:
mkdir -p airflow/dags, airflow/logs, airflow/plugins
mkdir -p datalake
mkdir -p docker-volume/db-data
```

### 3. Build da Imagem Spark Customizada
```powershell
# Construir imagem Spark com Delta Lake
docker build -t spark-delta:3.5.7 -f Dockerfile.spark-delta .
```

## ğŸš€ Subindo a Infraestrutura

### OpÃ§Ã£o 1: Subir Tudo de Uma Vez (Recomendado para Primeira ExecuÃ§Ã£o)
```powershell
docker-compose up -d
```

### OpÃ§Ã£o 2: Subir por Camadas (Para Debug)
```powershell
# 1. Camada de Storage
docker-compose up -d postgres mysql minio elasticsearch

# 2. Camada de Processamento
docker-compose up -d zookeeper kafka spark-master spark-worker

# 3. Camada de OrquestraÃ§Ã£o
docker-compose up -d airflow-initdb
docker-compose up -d airflow-webserver airflow-scheduler

# 4. Camada de GovernanÃ§a
docker-compose up -d execute-migrate-all openmetadata-server ingestion
```

### OpÃ§Ã£o 3: Subir ServiÃ§os EspecÃ­ficos
```powershell
# Apenas Spark + MinIO
docker-compose up -d spark-master spark-worker minio

# Apenas Airflow + PostgreSQL
docker-compose up -d postgres airflow-initdb airflow-webserver airflow-scheduler

# Apenas Kafka
docker-compose up -d zookeeper kafka
```

## ğŸ“Š VerificaÃ§Ã£o dos ServiÃ§os

### Comando de Status
```powershell
# Verificar todos os containers
docker-compose ps

# Verificar logs de um serviÃ§o especÃ­fico
docker-compose logs -f spark-master
docker-compose logs -f airflow-webserver
```

### Health Checks AutomÃ¡ticos
```powershell
# Aguardar todos os serviÃ§os ficarem healthy
docker-compose ps | Select-String "healthy"

# Verificar serviÃ§os especÃ­ficos
docker-compose exec spark-master curl -f http://localhost:8080
docker-compose exec airflow-webserver curl -f http://localhost:8080
```

## ğŸŒ Acessando as Interfaces

| ServiÃ§o | URL | UsuÃ¡rio | Senha |
|---------|-----|---------|--------|
| **Spark Master UI** | http://localhost:8080 | - | - |
| **Spark Worker UI** | http://localhost:8081 | - | - |
| **Airflow UI** | http://localhost:8082 | `admin` | `admin` |
| **MinIO Console** | http://localhost:9001 | `admin` | `password` |
| **OpenMetadata** | http://localhost:8585 | `admin` | `admin` |
| **Elasticsearch** | http://localhost:9200 | - | - |

## ğŸ”§ ConfiguraÃ§Ã£o Inicial

### 1. Configurar Buckets no MinIO
```powershell
# Acessar MinIO e criar buckets necessÃ¡rios:
# - bronze (para dados raw)
# - silver (para dados processados)
# - backup (para backups)
```

### 2. Verificar Airflow
```powershell
# Verificar se DAGs foram carregadas
curl http://localhost:8082/api/v1/dags

# Ou acessar a interface web em http://localhost:8082
```

### 3. Testar Spark
```powershell
# Executar script de teste Delta Lake
docker exec spark_master python3 /opt/spark/work-dir/delta_timetravel.py
```

## ğŸ“‹ Scripts ETL DisponÃ­veis

### Scripts Spark
```powershell
# Bronze Layer (IngestÃ£o de dados)
docker exec spark_master python3 /opt/spark/work-dir/bronze_script.py

# Silver Layer (TransformaÃ§Ãµes)
docker exec spark_master python3 /opt/spark/work-dir/silver_script.py

# Delta Lake Time Travel (DemonstraÃ§Ã£o)
docker exec spark_master python3 /opt/spark/work-dir/delta_timetravel.py
```

### DAGs do Airflow DisponÃ­veis
- `etl_cintilografia_pipeline` - Pipeline ETL original
- `etl_cintilografia_working_no_docker` - Pipeline sem dependÃªncia Docker
- `etl_spark_docker_real` - Pipeline com execuÃ§Ã£o real via Docker socket
- `etl_spark_http_execution` - Pipeline via HTTP requests

## ğŸ” Monitoramento e Debug

### Verificar Logs
```powershell
# Logs em tempo real
docker-compose logs -f [service-name]

# Logs dos Ãºltimos 100 linhas
docker-compose logs --tail=100 [service-name]

# Logs de todos os serviÃ§os
docker-compose logs
```

### Verificar Recursos
```powershell
# Uso de recursos por container
docker stats

# EspaÃ§o em disco usado
docker system df

# InformaÃ§Ãµes detalhadas dos containers
docker-compose ps -a
```

### Resolver Problemas Comuns

#### Problema: Container nÃ£o inicia
```powershell
# Verificar logs especÃ­ficos
docker-compose logs [service-name]

# Reiniciar serviÃ§o especÃ­fico
docker-compose restart [service-name]

# Recriar container
docker-compose up -d --force-recreate [service-name]
```

#### Problema: Portas ocupadas
```powershell
# Verificar portas em uso (Windows)
netstat -an | Select-String ":8080|:8082|:9000"

# Parar todos os containers
docker-compose down

# Limpar networks
docker network prune
```

#### Problema: Falta de memÃ³ria
```powershell
# Verificar uso de memÃ³ria
docker stats --no-stream

# Parar serviÃ§os nÃ£o essenciais temporariamente
docker-compose stop openmetadata-server ingestion elasticsearch
```

## ğŸ”„ Comandos de ManutenÃ§Ã£o

### Atualizar e Reiniciar
```powershell
# Baixar imagens atualizadas
docker-compose pull

# Parar tudo
docker-compose down

# Subir novamente
docker-compose up -d
```

### Limpeza do Sistema
```powershell
# Remover containers parados
docker-compose down --remove-orphans

# Limpeza completa (CUIDADO: Remove volumes)
docker-compose down -v
docker system prune -a

# Limpeza preservando dados
docker-compose down
docker container prune
docker image prune
```

### Backup dos Dados
```powershell
# Backup dos volumes importantes
docker run --rm -v intro-de_postgres-data:/data -v ${PWD}:/backup busybox tar czf /backup/postgres-backup.tar.gz -C /data .
docker run --rm -v intro-de_es-data:/data -v ${PWD}:/backup busybox tar czf /backup/elasticsearch-backup.tar.gz -C /data .
```

## ğŸ“ Estrutura de Arquivos

```
intro-de/
â”œâ”€â”€ ğŸ“„ docker-compose.yaml          # ConfiguraÃ§Ã£o principal
â”œâ”€â”€ ğŸ“„ Dockerfile.spark-delta       # Build Spark com Delta Lake
â”œâ”€â”€ ğŸ“ airflow/
â”‚   â”œâ”€â”€ ğŸ“ dags/                    # DAGs do Airflow
â”‚   â”œâ”€â”€ ğŸ“ logs/                    # Logs do Airflow
â”‚   â””â”€â”€ ğŸ“ plugins/                 # Plugins personalizados
â”œâ”€â”€ ğŸ“ spark/
â”‚   â”œâ”€â”€ ğŸ“„ bronze_script.py         # Script Bronze Layer
â”‚   â”œâ”€â”€ ğŸ“„ silver_script.py         # Script Silver Layer
â”‚   â””â”€â”€ ğŸ“„ delta_timetravel.py      # Demo Delta Lake
â”œâ”€â”€ ğŸ“ datalake/                    # Dados persistentes
â”‚   â””â”€â”€ ğŸ“ postgres/                # Dados PostgreSQL
â”œâ”€â”€ ğŸ“ docker-volume/               # Volumes Docker
â”‚   â””â”€â”€ ğŸ“ db-data/                 # Dados MySQL OpenMetadata
â””â”€â”€ ğŸ“ data/                        # Dados de exemplo
    â”œâ”€â”€ ğŸ“„ dados_airbnb.parquet
    â””â”€â”€ ğŸ“„ dados_cintilografia.csv
```

## ğŸ¯ Casos de Uso

### 1. Desenvolvimento de ETL
```powershell
# Subir apenas Spark + MinIO + Airflow
docker-compose up -d postgres airflow-initdb airflow-webserver airflow-scheduler spark-master spark-worker minio
```

### 2. DemonstraÃ§Ã£o Completa
```powershell
# Subir toda a infraestrutura
docker-compose up -d
```

### 3. Streaming de Dados
```powershell
# Subir Kafka + Spark
docker-compose up -d zookeeper kafka spark-master spark-worker minio
```

## â— Notas Importantes

1. **Primeira execuÃ§Ã£o**: Pode demorar 5-10 minutos para baixar todas as imagens
2. **Recursos**: Monitore o uso de CPU e memÃ³ria
3. **Dados persistentes**: Ficam salvos em volumes Docker
4. **Networks**: Todos os serviÃ§os estÃ£o na network `datalake-net`
5. **Security**: Esta configuraÃ§Ã£o Ã© para desenvolvimento/demonstraÃ§Ã£o

## ğŸ“ Suporte

### Comandos Ãšteis para Debug
```powershell
# Status detalhado
docker-compose ps --format table

# Inspecionar configuraÃ§Ã£o
docker-compose config

# Verificar networks
docker network ls
docker network inspect intro-de_datalake-net

# Executar comandos dentro dos containers
docker exec -it spark_master bash
docker exec -it airflow_webserver bash
```

### Problemas Conhecidos
- **Windows**: Pode precisar habilitar virtualizaÃ§Ã£o no BIOS
- **WSL2**: Configurar recursos adequados no Docker Desktop
- **Firewall**: Pode bloquear algumas portas (8080, 8082, 9000, etc.)

---

## ğŸš€ Quick Start

```powershell
# 1. Build da imagem Spark
docker build -t spark-delta:3.5.7 -f Dockerfile.spark-delta .

# 2. Subir infraestrutura
docker-compose up -d

# 3. Aguardar containers (5-10 minutos)
docker-compose ps

# 4. Acessar interfaces
# Airflow: http://localhost:8082 (admin/admin)
# Spark: http://localhost:8080
# MinIO: http://localhost:9001 (admin/password)

# 5. Testar pipeline
docker exec spark_master python3 /opt/spark/work-dir/bronze_script.py
```

**ğŸ‰ Pronto! Sua infraestrutura de Data Engineering estÃ¡ funcionando!**